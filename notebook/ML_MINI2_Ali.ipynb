{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29841f91-7f00-419c-bd55-ce05d3c57f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, BertTokenizer\n",
    "from xgboost import XGBClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71532ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !pip install xgboost\n",
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc923f53-8e02-4cc9-ae9d-e2af66ab64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0 sample, the original  tweet content is: @elephantbird Hey dear, Happy Friday to You  Already had your rice's bowl for lunch ?\n",
      "For 0 sample, the processed tweet content is: hey dear, happy friday to you already had your rice's bowl for lunch?\n",
      "For 1 sample, the original  tweet content is: Ughhh layin downnnn    Waiting for zeina to cook breakfast\n",
      "For 1 sample, the processed tweet content is: ughhh layin downnnn waiting for zeina to cook breakfast\n",
      "For 2 sample, the original  tweet content is: @greeniebach I reckon he'll play, even if he's not 100%...but i know nothing!! ;) It won't be the same without him. \n",
      "For 2 sample, the processed tweet content is: i reckon he'll play, even if he's not 100 %... but i know nothing!! ; ) it won't be the same without him.\n",
      "For 3 sample, the original  tweet content is: @vaLewee I know!  Saw it on the news!\n",
      "For 3 sample, the processed tweet content is: i know! saw it on the news!\n",
      "For 4 sample, the original  tweet content is: very sad that http://www.fabchannel.com/ has closed down. One of the few web services that I've used for over 5 years \n",
      "For 4 sample, the processed tweet content is: very sad that has closed down. one of the few web services that i've used for over 5 years\n",
      "For 5 sample, the original  tweet content is: @Fearnecotton who sings 'I Remember'? i alwaysss hear it on Radio 1 but never catch the artist \n",
      "For 5 sample, the processed tweet content is: who sings'i remember '? i alwaysss hear it on radio 1 but never catch the artist\n",
      "For 6 sample, the original  tweet content is: With God on ur side anything is possible.... \n",
      "For 6 sample, the processed tweet content is: with god on ur side anything is possible....\n",
      "For 7 sample, the original  tweet content is: @LoveSmrs why being stupid? \n",
      "For 7 sample, the processed tweet content is: why being stupid?\n",
      "For 8 sample, the original  tweet content is: Having delved back into the guts of Expression Engine, its a flexible CMS if you have to use it as a dev, not great for clients though \n",
      "For 8 sample, the processed tweet content is: having delved back into the guts of expression engine, its a flexible cms if you have to use it as a dev, not great for clients though\n",
      "For 9 sample, the original  tweet content is: @emoskank awww   take him with you!\n",
      "For 9 sample, the processed tweet content is: awww take him with you!\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "if not os.path.exists('../data/processed_data.csv'):\n",
    "    \n",
    "    df_raw = pd.read_csv('../data/Sentiment140.tenPercent.sample.tweets.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "    # Load a pre-trained tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        # Remove TAGs\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        # Replace more than one space with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # If text is an empty string, change it to a space\n",
    "        text = text if text != '' else ' '\n",
    "\n",
    "        # Tokenize using Hugging Face tokenizer\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "        # Convert tokens to string and remove special tokens\n",
    "        preprocessed_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "        return preprocessed_text\n",
    "\n",
    "    def preprocess_wrapper(args):\n",
    "        index, row = args\n",
    "        row['tweet_text_processed'] = preprocess_text(row['tweet_text'])\n",
    "        return row\n",
    "\n",
    "    # Apply preprocess_text() to each row using multiprocessing\n",
    "    num_processes = cpu_count()  # Number of CPU cores\n",
    "    print(f'{num_processes} cores are using to process the data for accelerating processing time.')\n",
    "\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        result = list(tqdm(pool.imap(preprocess_wrapper, df_raw.iterrows()), total=len(df_raw)))\n",
    "\n",
    "    # Convert the list of processed rows back to a DataFrame\n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv('../data/processed_data.csv', index=False)\n",
    "else:\n",
    "    df =  pd.read_csv('../data/processed_data.csv')\n",
    "\n",
    "# Print the original and processed tweet content for the first 10 samples\n",
    "for index, row in df.head(10).iterrows():\n",
    "    print(f\"For {index} sample, the original  tweet content is: {row['tweet_text']}\")\n",
    "    print(f\"For {index} sample, the processed tweet content is: {row['tweet_text_processed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f612d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 159733 entries, 0 to 159999\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   sentiment_label       159733 non-null  int64 \n",
      " 1   tweet_text            159733 non-null  object\n",
      " 2   tweet_text_processed  159733 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ceb1874-6ec8-47d7-a97a-55b2d8cb92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have df with 'tweet_text_processed' and 'binary_sentiment' columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tweet_text_processed'], df['sentiment_label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Use CountVectorizer to convert text to numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34180efd",
   "metadata": {},
   "source": [
    "## Regular Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52bf5014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Logistic Regression...\n",
      "Accuracy for Logistic Regression: 0.782139167996995\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78     15865\n",
      "           1       0.78      0.79      0.79     16082\n",
      "\n",
      "    accuracy                           0.78     31947\n",
      "   macro avg       0.78      0.78      0.78     31947\n",
      "weighted avg       0.78      0.78      0.78     31947\n",
      "\n",
      "--------------------------------------------------\n",
      "Training and evaluating Multinomial Naive Bayes...\n",
      "Accuracy for Multinomial Naive Bayes: 0.7705574858359158\n",
      "Classification Report for Multinomial Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.78     15865\n",
      "           1       0.79      0.74      0.76     16082\n",
      "\n",
      "    accuracy                           0.77     31947\n",
      "   macro avg       0.77      0.77      0.77     31947\n",
      "weighted avg       0.77      0.77      0.77     31947\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_and_evaluate_models(X_train_vectorized, X_test_vectorized, y_train_encoded, y_test_encoded):\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Multinomial Naive Bayes': MultinomialNB(),\n",
    "        # 'Support Vector Machine': SVC(kernel='linear'),\n",
    "        # 'Random Forest': RandomForestClassifier(),\n",
    "        # 'XGBoost': XGBClassifier()\n",
    "    }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training and evaluating {model_name}...\")\n",
    "        model.fit(X_train_vectorized, y_train_encoded)\n",
    "        predictions = model.predict(X_test_vectorized)\n",
    "\n",
    "        accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "        print(f'Accuracy for {model_name}: {accuracy}')\n",
    "        \n",
    "        print(f'Classification Report for {model_name}:\\n{classification_report(y_test_encoded, predictions)}')\n",
    "        print('-' * 50)\n",
    "\n",
    "# Assuming X_train, X_test, y_train, and y_test are already defined\n",
    "test_and_evaluate_models(X_train_vectorized, X_test_vectorized, y_train_encoded, y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312aa312",
   "metadata": {},
   "source": [
    "## Regular NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82819adc-c45c-4fd6-a78c-d218489a505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to PyTorch tensors\n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_test = torch.tensor(y_test.values)\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Set hyperparameters\n",
    "input_size = X_train_vectorized.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 2  \n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN(input_size, hidden_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "# Initialize tqdm with the total number of epochs\n",
    "progress_bar = tqdm(total=num_epochs)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Convert input data to PyTorch tensor\n",
    "    X_train_tensor = torch.tensor(X_train_vectorized.toarray(), dtype=torch.float32)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate on training set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train_tensor)\n",
    "        train_predictions = torch.argmax(train_outputs, dim=1).numpy()\n",
    "\n",
    "    # Calculate metrics for training set\n",
    "    train_accuracy = accuracy_score(y_train.numpy(), train_predictions)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Print or log the metrics for training set\n",
    "        progress_bar.update(5)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}: Training Accuracy: {train_accuracy}')\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test_vectorized.toarray(), dtype=torch.float32)\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_predictions = torch.argmax(test_outputs, dim=1).numpy()\n",
    "\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27d38b",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adbd33f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128000,) (32000,) (128000,) (32000,)\n",
      "<class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(type(X_train), type(X_test) , type(y_train), type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a893525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 10%|█         | 2/20 [45:12<6:46:53, 1356.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m inputs, attention_mask, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Move data to CUDA\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m inputs, attention_mask, labels \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     49\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set init_weights to False\n",
    "model.init_weights = False\n",
    "\n",
    "# Move model to CUDA\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "X_train_tokens = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "X_test_tokens = tokenizer(X_test.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_dataset = TensorDataset(X_train_tokens['input_ids'], \n",
    "                              X_train_tokens['attention_mask'], \n",
    "                              torch.tensor(y_train.values).to('cuda'))\n",
    "test_dataset = TensorDataset(X_test_tokens['input_ids'], \n",
    "                             X_test_tokens['attention_mask'], \n",
    "                             torch.tensor(y_test.values).to('cuda'))\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Move optimizer to CUDA manually\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to('cuda')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# Initialize tqdm with the total number of epochs\n",
    "progress_bar = tqdm(total=num_epochs)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        # Move data to CUDA\n",
    "        inputs, attention_mask, labels = inputs.to('cuda'), attention_mask.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Print or log the metrics for the training set\n",
    "        progress_bar.update(5)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}: Training Accuracy: {train_accuracy}')\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        # Move data to CUDA\n",
    "        inputs, attention_mask, labels = inputs.to('cuda'), attention_mask.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        outputs = model(inputs, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy for BERT: {accuracy}')\n",
    "print(f'Classification Report for BERT:\\n{classification_report(y_test, predictions)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0371bb5",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad7a033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/3994 [47:15<?, ?it/s, Loss=0.694]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_vectorized.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_vectorized.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take the output of the last time step\n",
    "        output = self.fc(lstm_out)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "input_size = X_train_tensor.size(1)\n",
    "hidden_size = 100\n",
    "output_size = 1\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.long())\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc='Testing', leave=False):\n",
    "        outputs = model(inputs.long())\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        predictions.extend(predicted.numpy())\n",
    "        true_labels.extend(labels.numpy())\n",
    "\n",
    "# Convert predictions to binary values\n",
    "predictions = (np.array(predictions) > 0.5).astype(int)\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
