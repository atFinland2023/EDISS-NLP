{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29841f91-7f00-419c-bd55-ce05d3c57f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "import spacy\n",
    "import re\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c71532ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc923f53-8e02-4cc9-ae9d-e2af66ab64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0 sample, the original  tweet content is: @elephantbird Hey dear, Happy Friday to You  Already had your rice's bowl for lunch ?\n",
      "For 0 sample, the processed tweet content is:  hey dear Happy Friday rice bowl lunch\n",
      "For 1 sample, the original  tweet content is: Ughhh layin downnnn    Waiting for zeina to cook breakfast\n",
      "For 1 sample, the processed tweet content is: ughhh layin downnnn wait zeina cook breakfast\n",
      "For 2 sample, the original  tweet content is: @greeniebach I reckon he'll play, even if he's not 100%...but i know nothing!! ;) It won't be the same without him. \n",
      "For 2 sample, the processed tweet content is:  reckon play 100% know will\n",
      "For 3 sample, the original  tweet content is: @vaLewee I know!  Saw it on the news!\n",
      "For 3 sample, the processed tweet content is:  know see news\n",
      "For 4 sample, the original  tweet content is: very sad that http://www.fabchannel.com/ has closed down. One of the few web services that I've used for over 5 years \n",
      "For 4 sample, the processed tweet content is: sad close web service 5 year\n",
      "For 5 sample, the original  tweet content is: @Fearnecotton who sings 'I Remember'? i alwaysss hear it on Radio 1 but never catch the artist \n",
      "For 5 sample, the processed tweet content is:  sing remember alwaysss hear Radio 1 catch artist\n",
      "For 6 sample, the original  tweet content is: With God on ur side anything is possible.... \n",
      "For 6 sample, the processed tweet content is: God ur possible\n",
      "For 7 sample, the original  tweet content is: @LoveSmrs why being stupid? \n",
      "For 7 sample, the processed tweet content is:  stupid\n",
      "For 8 sample, the original  tweet content is: Having delved back into the guts of Expression Engine, its a flexible CMS if you have to use it as a dev, not great for clients though \n",
      "For 8 sample, the processed tweet content is: having delve gut Expression Engine flexible CMS use dev great client\n",
      "For 9 sample, the original  tweet content is: @emoskank awww   take him with you!\n",
      "For 9 sample, the processed tweet content is:  awww \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../data/processed_data.csv'):\n",
    "    \n",
    "    df_raw = pd.read_csv('../data/Sentiment140.tenPercent.sample.tweets.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "    # Load spaCy English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Function to preprocess text\n",
    "    def preprocess_text(text):\n",
    "        # Parse the text using spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Lemmatize each token and remove stop words and punctuation\n",
    "        preprocessed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "        \n",
    "        # Remove URLs\n",
    "        preprocessed_text = re.sub(r'http\\S+', '', preprocessed_text)\n",
    "        # Remove TAGs\n",
    "        preprocessed_text = re.sub(r'@\\w+', '', preprocessed_text)\n",
    "        \n",
    "        # Replace more than one space with a single space\n",
    "        preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text)\n",
    "        \n",
    "        # If preprocessed_text is an empty string, change it to a space\n",
    "        preprocessed_text = preprocessed_text if preprocessed_text != '' else ' '\n",
    "        \n",
    "        return preprocessed_text\n",
    "\n",
    "    def preprocess_wrapper(args):\n",
    "        index, row = args\n",
    "        row['tweet_text_processed'] = preprocess_text(row['tweet_text'])\n",
    "        return row\n",
    "\n",
    "    # Apply preprocess_text() to each row using multiprocessing\n",
    "    num_processes = cpu_count()  # Number of CPU cores\n",
    "    print(f'{num_processes} cores are using to process the data for accelerating processing time.')\n",
    "\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        result = list(tqdm(pool.imap(preprocess_wrapper, df_raw.iterrows()), total=len(df_raw)))\n",
    "\n",
    "    # Convert the list of processed rows back to a DataFrame\n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv('../data/processed_data.csv', index=False)\n",
    "else:\n",
    "    df =  pd.read_csv('../data/processed_data.csv')\n",
    "\n",
    "# Print the original and processed tweet content for the first 10 samples\n",
    "for index, row in df.head(10).iterrows():\n",
    "    print(f\"For {index} sample, the original  tweet content is: {row['tweet_text']}\")\n",
    "    print(f\"For {index} sample, the processed tweet content is: {row['tweet_text_processed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f612d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160000 entries, 0 to 159999\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   sentiment_label       160000 non-null  int64 \n",
      " 1   tweet_text            160000 non-null  object\n",
      " 2   tweet_text_processed  160000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ceb1874-6ec8-47d7-a97a-55b2d8cb92e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_sentiment\n",
      "1    80000\n",
      "0    80000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['binary_sentiment'] = df['sentiment_label'].apply(lambda x: 1 if x == 4 else 0)\n",
    "# Count the frequency of each unique value in the 'sentiment_label' column\n",
    "frequency_count = df['binary_sentiment'].value_counts()\n",
    "# Print the result\n",
    "print(frequency_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef65e9c7-00be-4e48-8294-3d0b3b823f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tweet_text_processed'], df['binary_sentiment'], test_size=0.2, random_state=42)\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "# Convert text data into numerical features using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52bf5014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.750875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.75     16002\n",
      "           1       0.76      0.74      0.75     15998\n",
      "\n",
      "    accuracy                           0.75     32000\n",
      "   macro avg       0.75      0.75      0.75     32000\n",
      "weighted avg       0.75      0.75      0.75     32000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = nb_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "82819adc-c45c-4fd6-a78c-d218489a505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [29:07<00:00, 87.40s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:47<02:23,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: Training Accuracy: 0.7592109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:35<01:35,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: Training Accuracy: 0.8000703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [02:25<00:49,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: Training Accuracy: 0.80275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:14<00:00,  9.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: Training Accuracy: 0.805828125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74     16002\n",
      "           1       0.74      0.72      0.73     15998\n",
      "\n",
      "    accuracy                           0.73     32000\n",
      "   macro avg       0.73      0.73      0.73     32000\n",
      "weighted avg       0.73      0.73      0.73     32000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to PyTorch tensors\n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_test = torch.tensor(y_test.values)\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set hyperparameters\n",
    "input_size = X_train_vectorized.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 2  \n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "# Initialize tqdm with the total number of epochs\n",
    "progress_bar = tqdm(total=num_epochs)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Convert input data to PyTorch tensor\n",
    "    X_train_tensor = torch.tensor(X_train_vectorized.toarray(), dtype=torch.float32)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate on training set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train_tensor)\n",
    "        train_predictions = torch.argmax(train_outputs, dim=1).numpy()\n",
    "\n",
    "    # Calculate metrics for training set\n",
    "    train_accuracy = accuracy_score(y_train.numpy(), train_predictions)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Print or log the metrics for training set\n",
    "        progress_bar.update(5)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}: Training Accuracy: {train_accuracy}')\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test_vectorized.toarray(), dtype=torch.float32)\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_predictions = torch.argmax(test_outputs, dim=1).numpy()\n",
    "\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4910b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_label    0\n",
      "tweet_text         0\n",
      "dtype: int64\n",
      "sentiment_label           0\n",
      "tweet_text                0\n",
      "tweet_text_processed    914\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv('../data/Sentiment140.tenPercent.sample.tweets.tsv', sep='\\t')\n",
    "df_2 =  pd.read_csv('../data/processed_data.csv')\n",
    "print(df_1.isnull().sum())\n",
    "print(df_2.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
